# 共享数据结构访问/线程同步



对于32/64 bit机器，





如果写操作只需执行一条修改内存的指令就能完成对共享数据结构的修改，例如修改某些基本数据类型的变量char、short、int、指针(以arm架构为例，一条str指令就能完成对内存里面这些基本数据类型的值的修改)，则读操作**不需要**与写操作竞争锁，写操作之间**需要**竞争锁。这种情况读操作无需处理，只需在写操作之间使用互斥量来处理。如果只有一个线程进行写操作，则互斥量也可以去掉。

pthread_mutex_init

pthread_mutex_lock

pthread_mutex_unlock

pthread_mutex_destroy

mutex只有0，1两种状态。解锁的情况下再多次解锁，最后上锁时只有第一次上锁成功。









如果写操作需多条修改内存的指令才能完成对共享数据结构的修改，例如修改一个结构体或者几个变量需要同步修改，则读操作**需要**同写操作竞争锁，写操作之间**需要**竞争锁。这种情况所有读写操作之间使用读写锁来处理。即使只有一个线程写或只有一个线程读，也不能省掉读写锁，因为要处理读操作和写操作之间的竞争关系。

pthread_rwlock_init

pthread_rwlock_destroy

pthread_rwlock_rdlock

pthread_rwlock_wrlock

pthread_rwlock_unlock







# 条件量

条件量之线程同步，A线程设置x变量，B线程根据x变量是否为真来决定是否继续运行，如果x为假则B线程阻塞，当A线程让x为真时B线程继续运行，A线程通过x控制B线程的运行。A可读写x，B只读x，mutex仅用来防止：B在while满足调用pthread_cond_wait之前，A线程执行到x=1;pthread_cond_signal，此时pthread_cond_signal先于pthread_cond_wait执行，导致后面B执行pthread_cond_wait时无法解锁。

    #include<stdio.h>
    #include <unistd.h>
    #include<pthread.h>
    
    pthread_t pthread;
    pthread_mutex_t mutex;
    pthread_cond_t cond;
    int x=0;
    
    void *pthread_fn(void *ptr)
    {
            for(;;){
                    pthread_mutex_lock(&mutex);
                    while(!x)
                            pthread_cond_wait(&cond,&mutex);
                    pthread_mutex_unlock(&mutex);
                    printf("start\n");
            }
            return NULL;
    }
    
    int main(){
            pthread_mutex_init(&mutex,NULL);
            pthread_cond_init(&cond,NULL);
            pthread_create(&pthread, NULL, pthread_fn,NULL);
            sleep(3);
    		
    		for(;;) {
    			//do something
    
                x=1;//当x从0到1时需要调用pthread_cond_signal,x从1到1时可调也可不调
                pthread_cond_signal(&cond);
                sleep(100);
    		}
        return 0;
    }




条件量  A线程生产，B线程消费，队列为空一直消费线程阻塞等待，A线程控制B     A/B都能读写队列，因此A线程操作head/tail条件时要加锁， B线程同样要在加锁环境中操作head/tail条件，此时mutex锁有两层含义，一层是保护head/tail条件共享变量的访问，另一层则负责条件量之线程同步，以免出现上面说的pthread_cond_signal先于pthread_cond_wait的情况。

```
typedef struct msg {
    struct msg * next;
    void * info;
} msg_t;

typedef struct {
    pthread_cond_t msg_cond;
    pthread_mutex_t msg_mutex;
    msg_t * head;
    msg_t * tail;
} mynet_state;

//A线程调用enqueue_msg
void enqueue_msg(mynet_state * s, msg_t * msg)
{
    msg->next = NULL;
#if 1
    pthread_mutex_lock(&s->msg_mutex);
#else
	while(pthread_mutex_trylock(&s->msg_mutex));
#endif
    if(s->tail) {
        s->tail->next = msg;
        s->tail = msg;
    } else {
        s->head = msg;
        s->tail = msg;
    }
    pthread_mutex_unlock(&s->msg_mutex);
    pthread_cond_signal(&s->msg_cond);
}

//B线程调用dequeue_msg
msg_t * dequeue_msg(mynet_state * s)
{
    msg_t * ptr;
    pthread_mutex_lock(&s->msg_mutex);
    while(s->head==NULL){
        pthread_cond_wait(&s->msg_cond,&s->msg_mutex);
    }
    ptr = s->head;
    s->head = s->head->next;
    if(s->head==NULL){
        s->tail = NULL;
    }
    pthread_mutex_unlock(&s->msg_mutex);
    return ptr;
}
```









# 临界区上下文

无论是什么锁，在中断上下文中都不能休眠，如果调用加锁函数时需要阻塞休眠，则一定不能放中断上下文。



要特别注意临界区的两种情况，

1.当前CPU进入临界区后发生切换，切换后再次进入该临界区

2.当前CPU进入临界区，同时其他CPU也进入该临界区



在中断上半部，当前CPU的所有中断被关闭，该中断线被屏蔽

在软中断，当前CPU所有中断未被屏蔽，但禁止抢占，在执行过程中被硬件中断打断处理完中断上半部后再次回到被打断的软中断去执行，不会调度到其他线程。





# 自旋锁



<linux/spinlock.h>



spinlock_t my_lock = SPIN_LOCK_UNLOCKED;

void spin_lock_init(spinlock_t *lock);





void spin_lock(spinlock_t *lock);

中间不能休眠

void spin_unlock(spinlock_t *lock);





spin_lock在获取不到锁在等待时CPU在轮询，因此在获得锁到释放锁期间不能休眠，以免其他等待锁的CPU不断地轮询



中断上半部用spin_lock                线程上下文用spin_lock_irqsave

软中断用spin_lock                       线程上下文用spin_lock_bh

线程上下文用spin_lock               线程上下文用spin_lock



```
void spin_lock(spinlock_t *lock);
void spin_lock_irqsave(spinlock_t *lock, unsigned long flags);
void spin_lock_irq(spinlock_t *lock);
void spin_lock_bh(spinlock_t *lock);



int spin_trylock(spinlock_t *lock);
int spin_trylock_irqsave(spinlock_t *lock, unsigned long flags);
int spin_trylock_irq(spinlock_t *lock);
int spin_trylock_bh(spinlock_t *lock);


void spin_unlock(spinlock_t *lock);
void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags);
void spin_unlock_irq(spinlock_t *lock);
void spin_unlock_bh(spinlock_t *lock);
```







读写自旋锁

<linux/spinlokc.h>



* 写者之间互斥，仅允许一个进入临界区

* 写者与读者之间互斥，所有读者离开临界区后写着才能进入临界区，写者离开临界区后读者才能进入临界区

* 读者之间可并行

* 写者优先，当写者在临界区等待读者离开临界区时，如果后续又来了读者，则后续来的读者需要等待写者进入临界区离开临界区之后才能进入临界区，在有大量的写者来竞争的情况下会导致读者饥饿。

* 在不满足条件时CPU轮询等待。



```
rwlock_t my_rwlock = RW_LOCK_UNLOCKED; /* Static way */
rwlock_t my_rwlock;
rwlock_init(&my_rwlock); /* Dynamic way */

void read_lock(rwlock_t *lock);
void read_lock_irqsave(rwlock_t *lock, unsigned long flags);
void read_lock_irq(rwlock_t *lock);
void read_lock_bh(rwlock_t *lock);



void read_unlock(rwlock_t *lock);
void read_unlock_irqrestore(rwlock_t *lock, unsigned long flags);
void read_unlock_irq(rwlock_t *lock);
void read_unlock_bh(rwlock_t *lock);



void write_lock(rwlock_t *lock);
void write_lock_irqsave(rwlock_t *lock, unsigned long flags);
void write_lock_irq(rwlock_t *lock);
void write_lock_bh(rwlock_t *lock);


int write_trylock(rwlock_t *lock);
void write_unlock(rwlock_t *lock);
void write_unlock_irqrestore(rwlock_t *lock, unsigned long flags);
void write_unlock_irq(rwlock_t *lock);
void write_unlock_bh(rwlock_t *lock);
```













# semaphore



<linux/semaphore.h>





不区分读写版本

```text
//定义并初始化为1
DEFINE_SEMAPHORE(sem1);

//初始化为val
void sema_init(struct semaphore *sem, int val);


//获取信号量，只能在线程上下文中调用，若是获取不到进入 TASK_UNINTERRUPTIBLE 状态
void down(struct semaphore *sem);
//同 down()，区别是若获取不到进入 TASK_INTERRUPTIBLE 状态，支持被信号唤醒
int down_interruptible(struct semaphore *sem);
//同 down()，区别是若获取不到进入 TASK_KILLABLE(TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)状态，表示只可被 SIGKILL 信号唤醒
int down_killable(struct semaphore *sem);
//尝试获取信号量，不会阻塞，返回1成功，0失败
int down_trylock(struct semaphore *sem);
//同 down()，区别是若获取不到不会一直等待，而是有一个超时时间，到期后自动唤醒
int down_timeout(struct semaphore *sem, long jiffies);


//Release the semaphore.  Unlike mutexes, up() may be called from any context and even by tasks which have never called down().
void up(struct semaphore *sem);
```



<linux/rwsem.h>

读写信号量版本：

```
init_rwsem(struct rw_semaphore *sem)

void down_read(struct rw_semaphore *sem);
int down_read_trylock(struct rw_semaphore *sem);

void down_write(struct rw_semaphore *sem);
int down_write_trylock(struct rw_semaphore *sem);
void downgrade_write(struct rw_semaphore *sem);

void up_write(struct rw_semaphore *sem);
void up_read(struct rw_semaphore *sem);
```

写者有优先权; 当一个写者
试图进入临界区, 就不会允许读者进入直到所有的写者完成了它们的工作. 这个实现可能
导致读者饥饿 -- 读者被长时间拒绝存取 -- 如果你有大量的写者来竞争旗标. 由于这个
原因, rwsem 最好用在很少请求写的时候, 并且写者只占用短时间.

读写信号量中写入者优先级比读取者高，且排斥读取者，这就可能会出现在写入频繁时，读取者长时间获取不到锁。因此可以在某次写入者释放锁后，调用**downgrade_write**可在一段时间内禁止写入，即将保护资源变成了只读。









# Completions



<linux/completion.h>







对不可用的情况做了优化，spin/semaphore对可用的情况做了优化，如果不可用则去休眠

多于一个线程在等待同一个 completion 事件, 这 2 个函数做法不同. complete 只唤醒一个等待的线程, 而 complete_all 允许它们所有都继续，但使用complete_all后需重新初始化 completion 结构才能继续使用

```
定义初始化
DECLARE_COMPLETION(my_completion);

struct completion my_completion;
init_completion(&my_completion);


等待
void wait_for_completion(struct completion *c);//只能在线程上下文中调用

bool try_wait_for_completion(struct completion *x)


释放
void complete(struct completion *c);

void complete_all(struct completion *c);

//Test to see if a completion has any waiters
bool completion_done(struct completion *x)

//在module remve函数中wait_for_completion,在module创建的线程中调用complete_and_exit
void complete_and_exit(struct completion *comp, long code)
```





